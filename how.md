# Como o Sistema RSS Skull Funciona

## Vis√£o Geral

O RSS Skull √© um bot do Telegram que monitora feeds RSS e notifica os usu√°rios sobre novos itens. O sistema foi projetado para ser altamente resiliente e nunca parar de funcionar, mesmo em caso de erros ou falhas.

## Arquitetura do Sistema

### Componentes Principais

1. **Bot Service** (`src/bot/bot.service.ts`)
   - Gerencia a comunica√ß√£o com o Telegram
   - Processa comandos e mensagens
   - Gerencia o polling do Telegram API

2. **Feed Queue Service** (`src/jobs/feed-queue.service.ts`)
   - Gerencia filas de verifica√ß√£o de feeds RSS
   - Usa BullMQ com Redis para processamento ass√≠ncrono
   - Agenda verifica√ß√µes recorrentes de feeds

3. **Job Service** (`src/jobs/job.service.ts`)
   - Gerencia workers para processamento de jobs
   - Conecta-se ao Redis para filas
   - Processa jobs de verifica√ß√£o de feeds e envio de mensagens

4. **Database Service** (`src/database/database.service.ts`)
   - Gerencia conex√£o com SQLite (via Prisma)
   - Armazena feeds, configura√ß√µes e estado

5. **Resilience System** (`src/resilience/`)
   - Sistema de resili√™ncia para Telegram API
   - Circuit breakers, retry logic, filas de mensagens
   - Monitoramento de sa√∫de da conex√£o

## Preven√ß√£o de Sa√≠da do Processo

### O Problema

Anteriormente, o processo Node.js podia sair silenciosamente ap√≥s algum tempo, deixando apenas o Redis rodando. Isso acontecia porque:

1. Event loop vazio - quando n√£o havia mais tarefas ass√≠ncronas pendentes
2. Promises n√£o tratadas - causando crashes silenciosos
3. Erros n√£o capturados - fazendo o processo terminar

### A Solu√ß√£o Implementada

#### 1. Loop Infinito de Keep-Alive

O sistema implementa **3 intervalos permanentes** que nunca s√£o limpos (exceto durante graceful shutdown):

```typescript
// Intervalo prim√°rio - a cada 5 segundos
setInterval(() => {
  if (!gracefulShutdownFlag) {
    process.memoryUsage(); // Mant√©m event loop ativo
  }
}, 5000);

// Intervalo de heartbeat - a cada 30 segundos
setInterval(() => {
  // Loga status e mant√©m processo vivo
}, 30000);

// Intervalo de seguran√ßa - a cada 10 segundos
setInterval(() => {
  // Opera√ß√£o ass√≠ncrona adicional
}, 10000);
```

Estes intervalos garantem que **o event loop nunca fique vazio**, prevenindo que o processo saia automaticamente.

#### 2. Preven√ß√£o Real de Sa√≠da

O sistema intercepta o evento `beforeExit` e **previne ativamente** a sa√≠da:

```typescript
process.on('beforeExit', (code) => {
  if (gracefulShutdownFlag) {
    return; // Permite sa√≠da apenas em graceful shutdown
  }
  
  // PREVINE SA√çDA criando novas tarefas ass√≠ncronas
  setImmediate(() => { /* ... */ });
  setTimeout(() => { /* ... */ }, 0);
  process.nextTick(() => { /* ... */ });
});
```

Isso cria novas tarefas no event loop sempre que o Node.js tenta sair, mantendo o processo vivo.

#### 3. Tratamento de Promises

Todas as promises n√£o tratadas s√£o capturadas e **nunca causam sa√≠da**:

```typescript
process.on('unhandledRejection', (reason, promise) => {
  // Intercepta e trata - N√ÉO permite que o processo saia
  errorRecoveryService.interceptUnhandledRejection(error, promise);
  // Processo continua rodando
});
```

A fun√ß√£o `safeAsync()` envolve opera√ß√µes cr√≠ticas para garantir que nunca haja promises n√£o tratadas.

#### 4. Bootstrap com Retry

Se o bootstrap falhar, o sistema tenta novamente com backoff exponencial:

```typescript
async function bootstrapWithRetry() {
  let attempt = 0;
  let delay = 5000;
  
  while (attempt < MAX_BOOTSTRAP_ATTEMPTS) {
    try {
      await bootstrap();
      return; // Sucesso
    } catch (error) {
      // Retry com backoff exponencial
      delay = Math.min(delay * 2, 60000);
    }
  }
  
  // Mesmo se todas as tentativas falharem, o processo permanece vivo
  setupKeepAlive();
}
```

## Service Watchdog

O sistema inclui um **watchdog** que monitora continuamente os servi√ßos cr√≠ticos:

```typescript
class ServiceWatchdog {
  // Verifica sa√∫de dos servi√ßos a cada 30 segundos
  checkServicesHealth() {
    // Verifica: Database, Redis, Server, Bot Polling
    // Se algum falhar 3 vezes consecutivas, tenta recuperar
  }
}
```

### Servi√ßos Monitorados

1. **Database** - Verifica conex√£o e health check
2. **Redis** - Verifica conex√£o e ping
3. **HTTP Server** - Verifica se est√° escutando
4. **Bot Polling** - Verifica se o polling est√° ativo

### Recupera√ß√£o Autom√°tica

Se um servi√ßo falhar:
- Tenta reconectar automaticamente (Redis, Database)
- Reinicia o polling do bot se necess√°rio
- Loga todos os eventos para diagn√≥stico

## Monitoramento do Bot Polling

O sistema verifica continuamente se o polling do Telegram est√° ativo:

```typescript
async isPollingActive(): Promise<boolean> {
  // Tenta obter informa√ß√µes do bot
  await this.bot.api.getMe();
  return true; // Se funcionar, polling est√° ativo
}

async restartPollingIfNeeded(): Promise<boolean> {
  // Para o polling existente
  // Limpa webhooks
  // Reinicia com grammY Runner ou polling padr√£o
}
```

O watchdog chama este m√©todo automaticamente se detectar que o polling parou.

## Sistema de Resili√™ncia

### Componentes

1. **Connection Manager**
   - Monitora estado da conex√£o com Telegram
   - Persiste estado de conex√£o no banco
   - Detecta desconex√µes e tenta reconectar

2. **Circuit Breaker**
   - Previne chamadas excessivas quando API est√° inst√°vel
   - Abre/fecha automaticamente baseado em falhas

3. **Message Queue**
   - Enfileira mensagens quando API falha
   - Processa fila quando API volta a funcionar
   - Prioriza mensagens importantes

4. **Health Monitor**
   - Monitora m√©tricas de sa√∫de
   - Detecta padr√µes de falha
   - Gera alertas quando necess√°rio

## Fluxo de Processamento

### 1. Inicializa√ß√£o

```
bootstrapWithRetry()
  ‚Üì
bootstrap()
  ‚Üì
- Inicializa Database
- Inicia HTTP Server (Fastify)
- Inicializa Redis Connection
- Inicia Bot Service
- Carrega feeds do banco
- Agenda verifica√ß√µes recorrentes
  ‚Üì
setupKeepAlive() - CR√çTICO: mant√©m processo vivo
watchdog.start() - Monitora servi√ßos
```

### 2. Processamento de Feeds

```
Feed Queue Service
  ‚Üì
Agenda job recorrente (ex: a cada 5 minutos)
  ‚Üì
Worker processa job
  ‚Üì
Fetch RSS feed
  ‚Üì
Detecta novos itens
  ‚Üì
Cria jobs de notifica√ß√£o
  ‚Üì
Message Sender Processor
  ‚Üì
Envia mensagens via Telegram
```

### 3. Tratamento de Erros

```
Erro ocorre
  ‚Üì
Error Recovery Service intercepta
  ‚Üì
Classifica erro (recoverable/non-recoverable)
  ‚Üì
Se recoverable: agenda retry
  ‚Üì
Se non-recoverable: apenas loga
  ‚Üì
Processo CONTINUA rodando (n√£o sai)
```

## Logs e Monitoramento

### Heartbeat

A cada 30 segundos, o sistema loga:

```
üíì HEARTBEAT - Process running for X minutes, Memory: YMB, PID: Z
```

Isso confirma que o processo est√° vivo e ativo.

### Logs de Status

- `üîí Keep-alive setup complete` - Confirma que keep-alive est√° ativo
- `üíì HEARTBEAT` - Processo est√° vivo
- `‚ö†Ô∏è BEFORE EXIT DETECTED` - Tentativa de sa√≠da foi prevenida
- `üõë GRACEFUL SHUTDOWN INITIATED` - Shutdown intencional iniciado

### Endpoints HTTP

- `GET /health` - Health check completo (usado pelo Docker)
- `GET /stats` - Estat√≠sticas gerais do sistema
- `GET /resilience-stats` - Status do sistema de resili√™ncia
- `GET /metrics` - M√©tricas detalhadas

## Docker e Deploy

### Configura√ß√£o Docker Compose

```yaml
restart: always  # Sempre reinicia se falhar
healthcheck:
  interval: 30s
  timeout: 10s
  retries: 10
  start_period: 60s
```

### Restart Policy

- **`restart: always`** - Docker sempre reinicia o container se ele parar
- Healthcheck verifica se o servi√ßo est√° respondendo
- Se healthcheck falhar 10 vezes consecutivas, Docker reinicia

## Graceful Shutdown

O processo s√≥ sai quando h√° um **shutdown graceful expl√≠cito**:

1. Recebe SIGINT ou SIGTERM
2. Define `gracefulShutdownFlag = true`
3. Para todos os intervalos de keep-alive
4. Para watchdog e servi√ßos
5. Para bot, fecha conex√µes
6. Fecha HTTP server
7. Desconecta database
8. **Agora sim** chama `process.exit(0)`

## Por Que o Sistema N√£o Para Mais

1. **M√∫ltiplos intervalos** mant√™m o event loop permanentemente ativo
2. **Intercepta√ß√£o de beforeExit** previne sa√≠das n√£o intencionais
3. **Tratamento de promises** previne crashes silenciosos
4. **Retry autom√°tico** recupera de falhas tempor√°rias
5. **Watchdog** monitora e reinicia servi√ßos automaticamente
6. **Docker restart policy** garante rein√≠cio se o container parar
7. **Logs verbosos** facilitam diagn√≥stico

## Troubleshooting

### Processo ainda est√° parando?

1. Verifique logs para `BEFORE EXIT DETECTED` - deve prevenir
2. Verifique se `keep-alive setup complete` apareceu nos logs
3. Verifique heartbeat a cada 30 segundos
4. Verifique se h√° erros n√£o tratados nos logs

### Servi√ßos n√£o est√£o funcionando?

1. Watchdog deve detectar e tentar recuperar
2. Verifique logs do watchdog para tentativas de recupera√ß√£o
3. Health check endpoint (`/health`) mostra status de cada servi√ßo

### Bot n√£o est√° respondendo?

1. Watchdog verifica polling a cada 30 segundos
2. Se polling parar, tenta reiniciar automaticamente
3. Verifique logs para `Bot polling is not active - attempting restart`

## Conclus√£o

Este sistema foi projetado para ser **extremamente resiliente**. Atrav√©s de m√∫ltiplas camadas de prote√ß√£o (keep-alive, intercepta√ß√£o de eventos, tratamento de erros, watchdog, Docker restart), o processo deve **nunca sair silenciosamente**. Se algo der errado, o sistema tenta recuperar automaticamente ou permanece vivo para interven√ß√£o manual.

